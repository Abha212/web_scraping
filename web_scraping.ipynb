{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CSV file in which scraped data is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '/Users/abha/Desktop/GitHub/Lending Club'\n",
    "#directory = './GitHub/Lending Club'\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError:\n",
    "    if not os.path.isdir(directory):\n",
    "        raise\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am only focusing on Notes that have been issued and sold prior to 12-31-2010. However, the analysis can be easily extented for entire sample period uptil 8-31-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filing_links(url):\n",
    "    'Function that identifies and collects all hyperlinks displayed on URL into a list'\n",
    "    r = requests.get(url)\n",
    "    links = re.findall('<filingHREF>(.*?)</filingHREF>', r.text)\n",
    "    links_new = [link.replace('-index.htm', '.txt') for link in links]\n",
    "    return (links_new)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sales_filing(url):\n",
    "    'Function that checks if URL is to a sales filing or not'\n",
    "    response = requests.get(url) #FILENAME is either salessup or postingsup\n",
    "    if re.search('salessup', response.text) is not None:\n",
    "        print('sales prospectus found')\n",
    "        success = 1\n",
    "    else:\n",
    "        success=0\n",
    "    return (success)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scraped_data(url):\n",
    "    '''Function that scrapes info on borrower FICO scores, loan request date and final issuance date \n",
    "    for each note sold in Sales filings'''\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = []\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    for tag in soup.find_all(text=re.compile('Series of Member Payment Dependent Notes')):\n",
    "        if tag.findParent('table') is not None:\n",
    "            table = tag.findParent('table')\n",
    "            content = table.findNext('tr').findNext('tr').get_text().split('\\n\\n')[1:]\n",
    "            content = [s.rstrip() for s in content]\n",
    "            header = ['Series of Member Payment Dependent Notes', 'Aggregate principal amount of Notes offered', \n",
    "                      'Aggregate principal amount of Notes sold', 'Stated interest rate', 'Service charge', \n",
    "                      'Sale and Original Issue Date', 'Initial Maturity', 'Final Maturity', \n",
    "                      'Amount of corresponding member loan funded by Lending Club']\n",
    "            data.append(dict(zip(header, content))) #create list of dictionaries\n",
    "    for tag in soup.find_all(text=re.compile('Credit Score Range:')):\n",
    "        if tag.findParent('table') is not None:\n",
    "            table = tag.findParent('table')\n",
    "            content = table.tr.findNext('td').findNext('td').get_text()\n",
    "            header = ['Credit score range']\n",
    "            data1.append(dict(zip(header, [content])))\n",
    "    for tag in soup.find_all(text=re.compile(r'was requested on.*by a borrower')): \n",
    "            content = [re.findall('was requested on (.*?) by', tag),\n",
    "                       url.replace('.txt', '-index.htm')]\n",
    "            header = ['Request date', 'File name']\n",
    "            data2.append(dict(zip(header, content)))       \n",
    "    return (data, data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening CSV file 1 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 2 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 3 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 4 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 5 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 6 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 7 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 8 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 9 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 10 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 11 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 12 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 13 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 14 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "Opening CSV file 15 for writing!\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n",
      "sales prospectus found\n"
     ]
    }
   ],
   "source": [
    "cik = \"0001409970\"  \n",
    "sec_filing = \"424B3\"\n",
    "priorto = \"20101231\"\n",
    "count = 100\n",
    "for file_num in range(1, 16):\n",
    "    print('Opening CSV file %d for writing!' %file_num)\n",
    "    file_name = 'Sales Report %d.csv' %(file_num)\n",
    "    output = open(file_name, 'w')\n",
    "    start = (file_num - 1)*100\n",
    "    payload = {'CIK': cik, 'type': sec_filing, 'dateb':priorto, 'owner':'exclude', 'start': str(0), \n",
    "          'count':str(count), 'output':'xml'}\n",
    "    base_url = requests.get('https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany', params=payload).url\n",
    "    filing_links = get_filing_links(base_url)\n",
    "    ctr = 0\n",
    "    for url in filing_links:\n",
    "        success = get_sales_filing(url)\n",
    "        if success ==1:\n",
    "            ctr += 1\n",
    "            (data, data1, data2) = get_scraped_data(url)\n",
    "            if ctr==1:\n",
    "                df = pd.concat([pd.concat([pd.DataFrame(data), pd.DataFrame(data1)], axis=1), pd.DataFrame(data2)], axis=1)\n",
    "            else:\n",
    "                a = pd.concat([pd.concat([pd.DataFrame(data), pd.DataFrame(data1)], axis=1), pd.DataFrame(data2)], axis=1)\n",
    "                df = pd.concat([df, a])         \n",
    "    df.to_csv(file_name, index=False)\n",
    "    output.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
