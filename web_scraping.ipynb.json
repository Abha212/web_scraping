{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import re \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CSV file in which scraped data is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '/Users/abha/Desktop/GitHub/Lending Club'\n",
    "#directory = './GitHub/Lending Club'\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError:\n",
    "    if not os.path.isdir(directory):\n",
    "        raise\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am only focusing on Notes that have been issued and sold prior to 12-31-2010. However, the analysis can be easily extented for entire sample period uptil 8-31-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filing_links(url):\n",
    "    'Function that identifies and collects all hyperlinks displayed on URL into a list'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser') \n",
    "    tags = soup.find_all('filinghref')\n",
    "    links = []\n",
    "    for tag in tags:\n",
    "        a = tag.get_text()\n",
    "        #data scraped from txt files instead of htm files for Lending Club's SEC filings\n",
    "        a = a.replace('-index.htm', '.txt')\n",
    "        links.append(a)\n",
    "    return (links)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sales_filing(url):\n",
    "    'Function that checks if URL is to a sales filing or not'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    content = soup.find_all('table')[0].get_text()\n",
    "    if content.find('Prospectus Supplement (Sales Report)')>0:\n",
    "        print('Sales prospectus found')\n",
    "        success = 1\n",
    "    else:\n",
    "        success = 0\n",
    "    return (success)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scraped_data(url):\n",
    "    '''Function that scrapes info on borrower FICO scores, loan request date and final issuance date \n",
    "    for each note sold in Sales filings'''\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = []\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    for tag in soup.find_all(text=re.compile('Series of Member Payment Dependent Notes')):\n",
    "            table = tag.findParent('table')\n",
    "            table_row = table.find_all('tr')[1]\n",
    "            content = table_row.get_text().split('\\n\\n')[1:]\n",
    "            content = [s.rstrip() for s in content]\n",
    "            header = ['Series of Member Payment Dependent Notes', 'Aggregate principal amount of Notes offered', \n",
    "                      'Aggregate principal amount of Notes sold', 'Stated interest rate', 'Service charge', \n",
    "                      'Sale and Original Issue Date', 'Initial Maturity', 'Final Maturity', \n",
    "                      'Amount of corresponding member loan funded by Lending Club']\n",
    "            data.append(dict(zip(header, content))) #create list of dictionaries\n",
    "    for tag in soup.find_all(text=re.compile('Credit Score Range:')):\n",
    "            table = tag.findParent('table')\n",
    "            table_row = table.find_all('tr')[0]\n",
    "            if len(table_row.get_text().split('\\n'))==5:\n",
    "                content = table_row.get_text().split('\\n')[2]\n",
    "                header = ['Credit score range']\n",
    "                data1.append(dict(zip(header, [content]))) #create list of dictionaries  \n",
    "    for tag in soup.find_all(text=re.compile(r'was requested on.*by a borrower')):\n",
    "            a = tag.split('\\n')[1].split('. ')[1]\n",
    "            content = [' '.join(a.split(' ')[6:9]), url.replace('.txt', '-index.htm')]\n",
    "            header = ['Request date', 'File name']\n",
    "            data2.append(dict(zip(header, content)))       \n",
    "    return (data, data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cik = \"0001409970\"  \n",
    "sec_filing = \"424B3\"\n",
    "priorto = \"20101231\"\n",
    "count = 100\n",
    "for file_num in range(1, 16):\n",
    "    print('Opening CSV file %d for writing!' %file_num)\n",
    "    file_name = 'Sales Report %d.csv' %(file_num)\n",
    "    output = open(file_name, 'w')\n",
    "    start = (file_num - 1)*100\n",
    "    base_url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\"+str(cik)+\"&type=\"+str(sec_filing)+\"&dateb=\"+str(priorto)+\"&owner=exclude&start=\"+str(start)+\"&count=\"+str(count)+\"&output=xml\"\n",
    "    filing_links = get_filing_links(base_url)\n",
    "    ctr = 0\n",
    "    for url in filing_links:\n",
    "        success = get_sales_filing(url)\n",
    "        if success ==1:\n",
    "            ctr += 1\n",
    "            (data, data1, data2) = get_scraped_data(url)\n",
    "            if ctr==1:\n",
    "                df = pd.concat([pd.concat([pd.DataFrame(data), pd.DataFrame(data1)], axis=1), pd.DataFrame(data2)], axis=1)\n",
    "                print(df.shape)\n",
    "                print(url)\n",
    "            else:\n",
    "                a = pd.concat([pd.concat([pd.DataFrame(data), pd.DataFrame(data1)], axis=1), pd.DataFrame(data2)], axis=1)\n",
    "                print(a.shape)\n",
    "                print(url)\n",
    "                df = pd.concat([df, a])\n",
    "                print(df.shape)\n",
    "                \n",
    "    df.to_csv(file_name, index=False)\n",
    "    output.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
